<!DOCTYPE html>
<html lang="en">
    <head>
        <title>Audio Spatializer</title>
        <link rel="stylesheet" type="text/css" href="style.css">
    </head>

    <body>

        <div class="navbar">
            <a class="active" href="#">Home</a>
            <a href="#intro">Introduction</a>
            <a href="#design1">High Level Design</a>
            <a href="#design2">Program/hardware design</a>
            <a href="#results">Results</a>
            <a href="#conclusions">Conclusions</a>
            <a href="#appendix">Appendix</a>
        </div>

        <header>
            <div class="title">
                <h1>Audio Spatializer</h1>
                <h2>Bianca Tseng, Crystal Shi, Rosie Wildermuth</h2>
            </div>
        </header> 

        <section id="main-content">

            <p style="color:#ffffff">
                ECE 4760 Microcontrollers Fall 2022 Final Project
            </p>

            <a id="intro" class="anchor" href="intro" aria-hidden="true">
                <span aria-hidden="true" class="octicon octicon-link"></span></a>
            <br><hr><br>
            <h1>Project Introduction</h1>

            <pre>
Our project is a real time spatial audio simulator that allows the user to adjust their aural experience by changing the location and distance of a virtual audio source.

Inspired by the concept of 8D audio, we chose to implement a user controlled spatial audio system. 8D audio is a type of audio effect where, when wearing headphones, sounds seem to be coming from different locations in the room. Usually, this effect is implemented on specific songs and heavily edited to convince the user of the locational aspects of the song. We approached an alternative to this, where the user could control the location of a “virtual” audio source. By just putting on a pair of headphones, the user is immersed into an aural landscape that is under their complete control. The user is able to interface with a GUI to adjust the distance, angle, volume, and filtering of the audio they are inputting into the system in real time. 

</pre>

<a id="design1" class="anchor" href="design1" aria-hidden="true">
                <span aria-hidden="true" class="octicon octicon-link"></span></a>
            <br><hr><br>
            <h1>High level design</h1>
<pre>
In recent years, spatial audio has had a grip on short video viewing platforms such as TikTok through frequent viral videos in which viewers were told “put on headphones for the full experience”. While these videos are interesting to experience, what is even more interesting is the implementation of this concept. Inspired by these videos, we decided to create our own version of spatial audio, but adapt it to be able to be experienced in real time with any audio source the user wanted. Originally we began our project wanting to have the system track the user’s head and change the sound based on positioning. But because of hardware and time constraints, a GUI was used instead to mimic head movement. This turned out to be a great decision as it makes the final product more modular.
background math
logical structure
hardware/software tradeoffs

One of the issues that we have to keep in mind when demonstrating our project is music copyright law. Because of the nature of our project, any audio track can be played through our system. While this provides great flexibility for our users, it also can be a source of copyright infringement if the demonstration is recorded. Thus, we have been very careful when including the audio source in videos. Other than that, there are no other obvious patent, copyright, or trademark issues in our project.

</pre>
<a id="design2" class="anchor" href="design2" aria-hidden="true">
    <span aria-hidden="true" class="octicon octicon-link"></span></a>
<br><hr><br>
<h1>Program/hardware design</h1>
<pre>
program details. What parts were tricky to write?

In order to be able to take any audio source as an input to the project, we needed to be able to sample audio sources through the analog to digital converter (ADC) on the Raspberry Pi Pico. To do this the audio jack takes in the audio voltage inputs. This input is then passed through a high-pass biasing filter, which shifts the voltage waveforms up to contain only positive voltage components. This was done because the ADC can only sample positive waveforms. Anything fed to the ADC that has a negative value will be set to zero. While this seems relatively minor, this zeroing of values created static in the background of the audio that was quite noticeable. By shifting the voltage waves to center around 3.3 Volts instead of 0, static was able to be minimized. Finally, the voltage input was put through a low pass filter with a cutoff frequency of approximately 5kHz. This was done to filter out aliasing of relevant frequencies. Because of the limited RAM capabilities of the RP2040, we did not sample at all frequencies. This created aliasing of the unsampled frequencies in the audible range that distorted the audio. By implementing a low pass filter with a cutoff frequency at approximately the maximum frequency sampled, we were able to diminish the volume of the aliasing to lessen the distortion. The audio input was then sampled through the ADC which allowed us to have frequently updating audio input.

After the audio processing was complete on the RP2040, the voltage signals were output through I2C to a digital to analog converter (DAC). This converter was an external chip to the Raspberry Pi Pico who’s pinout can be found in Appendix C. The two audio outputs of the DAC were wired to the two output pins of the audio jack. Each output pin on the audio jack connects to a single ear of the headphones that are plugged in. Thus, each ear was able to have a unique sound profile allowing for a more comprehensive aural experience. 

The final hardware component of this project was the Video Graphics Array (VGA) which was used to display the FFT data, Audio signal input, and for debugging processes during code development. The wiring arrangement can be found in Appendix C. The three color pins were connected through 330 Ohm resistors to create a voltage divider to keep the output voltage in a usable range. Other than the three color pins, the ground, HSYNC, and VSYNC were also connected to the VGA output to display the visual outputs of our project.

Outside of the code provided by previous labs of this course, we did not use or reference any other designs or code from any other source. All of the algorithms we used were developed by either ourselves or the ECE 4760 Course Staff.
Things you tried which did not work
</pre>
<a id="results" class="anchor" href="results" aria-hidden="true">
    </a>
<br><hr><br>
<h1>Results of the design</h1>
<pre>
Any and all test data, scope traces, waveforms, etc
speed of execution (hesitation, filcker, interactiveness, concurrency)
accuracy (numeric, music frequencies, video signal timing, etc)

There are very minimal safety concerns for this design. One safety concern for this project was that the circuit was laid out on a breadboard, not a PCB. This means that all of the leads and wire connections are exposed to the user. It is possible that a user could discharge a capacitor if they are not careful. To mitigate this risk, the amount of exposed wire was lessened by keeping the leads of the capacitors and resistors quite short. Additionally, the size of the capacitors was small so that if they were to discharge, the voltage level would remain low. Other than this small risk, there are no major safety concerns that the user faces.

One of the main concerns that we focused on during the development of this project was its usability by a person outside of the development process. This came into play mostly during the writing and design of the graphical user interface (GUI). Ideally, any user should be able to intuitively understand the functions in the GUI without having to be educated on its functions. To ensure this in our project, we picked colors that would be intuitive to user interaction. Additionally, we labeled the left and right of the user so they would understand the layout of the environment around them. A series of concentric circles were added to the design around the user to visually indicate distance measurements. Finally, to more visually display the distance from the audio source to the user, an automatically adjusting line between the two icons was included. We found that this was the best way to design our GUI to be user friendly. Outside of the user interface, the only other portion of the design that the user has to interact with is plugging in an audio source and headphones. Any headphones with an audio jack can be plugged directly into the audio jack that is in the breadboard. This allows for great flexibility in the headphone preference of the user. Additionally, any audio source that can be plugged into an audio jack can be used, which includes most laptops. While this does not include Apple iphones, adapters can be used to interface with most phones. The ability to use most audio sources and audio outputs makes our project very easy to use and flexible to most user needs.

</pre>
<a id="conclusions" class="anchor" href="conclusions" aria-hidden="true">
    <span aria-hidden="true" class="octicon octicon-link"></span></a>
<br><hr><br>
<h1>Conclusions</h1>
<pre>
Analyze your design in terms of how the results met your expectations. What might you do differently next time?
How did your design conform to the applicable standards?

There are no intellectual property considerations in the design of this project. The only code sources that we used to develop our algorithm were provided through the course content and other open source libraries. Because our project is a mimic of a pre-existing audio adaptation technique, there is likely not a patentable idea within it.

</pre>
<a id="appendix" class="anchor" href="appendix" aria-hidden="true">
    <span aria-hidden="true" class="octicon octicon-link"></span></a>
<br><hr><br>
<h1>Appendix</h1>

<h2>Appendix A: Permissions</h2>
<pre>
"The group approves this report for inclusion on the course website.
"The group approves the video for inclusion on the course youtube channel."
<br></pre>

<h2>Appendix B: Commented program listings</h2>
<pre>
TODO: (pull from drive, or link git if we upload)
<br></pre>

<h2>Appendix C: Hardware Schematic</h2>
<pre>
TODO: insert schematic here 
<br></pre>

<h2>Appendix D: Responsibility Breakdown:</h2>
<pre>
Bianca: Software Design, Audio Adaptation Algorithm Development
Crystal: Software Design, GUI Design and Implementation
Rosie: Hardware Design, Software and Hardware Integration
<br></pre>

<h2>Appendix E: Github</h2>
<pre>
https://github.com/rosiewildermuth/ECE4760_FinalProject_bat66_cys37_rjw252
<br></pre>

<h2>Appendix F: References</h2> 
<pre>
Pi Pico Datasheet: https://datasheets.raspberrypi.com/pico/pico-datasheet.pdf
RP2040 SDK Guide: https://datasheets.raspberrypi.com/pico/raspberry-pi-pico-c-sdk.pdf
VGA Driver Page: https://vanhunteradams.com/Pico/VGA/VGA.html
DAC Datasheet: https://vanhunteradams.com/Pico/Cricket/DAC.pdf
ECE 4760 Lab 1: https://vanhunteradams.com/Pico/Cricket/Crickets.html
**include any other resources we used
<br></pre>



        </section>

        <div class="footer">
            <p>Footer</p>
        </div>

    </body>
</html>
